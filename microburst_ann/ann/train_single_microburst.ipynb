{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Microburst Artifitial Neural Network (ANN)\n",
    "To identify single microbursts in the SAMPEX 20 ms data. The training data comes from the microbursts identified with the O'Brien et al.'s, 2003 burst parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import microburst_ann.config as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify train and test data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = pathlib.Path(config.PROJECT_DIR, 'data', 'train.csv')\n",
    "test_path = pathlib.Path(config.PROJECT_DIR, 'data', 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv files\n",
    "train_df = pd.read_csv(train_path, index_col=0)\n",
    "test_df = pd.read_csv(test_path, index_col=0)\n",
    "# Drop the NaN rows\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "# Pop off the training and test labels and make them into their own pd.DataFrames\n",
    "train_labels = train_df.pop('label')\n",
    "test_labels = test_df.pop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pd.DataFrame -> tf.Dataset\n",
    "This streamlines the data input into Tensorflow and Karas. Now sure what else this is good for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_df.to_numpy(), train_labels.to_numpy())\n",
    "    )\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df.to_numpy(), test_labels.to_numpy())\n",
    "    )\n",
    "shuffled_train_dataset = train_dataset.shuffle(train_df.shape[0]).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ann_model():\n",
    "    \"\"\" \n",
    "    Specify the ANN model architecture and compile it.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(50,)),\n",
    "        tf.keras.layers.Dense(25, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_1 (Flatten)          (None, 50)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 25)                1275      \n_________________________________________________________________\ndense_4 (Dense)              (None, 10)                260       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 10)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 1,546\nTrainable params: 1,546\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_ann_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 1546 parameters to train. And now lets set up the checkpoint directory to save the model parameters to a binary (h5) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = config.PROJECT_DIR / 'ann' / 'model' / 'model.cp'\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(shuffled_train_dataset, # Normally, you would indicate the data and labels. However, the tf.Dataset takes care of the input.\n",
    "                    validation_data=(test_df.to_numpy(), test_labels), # Test dataset used to calculate the model accuracy.\n",
    "                    epochs=3, # How many epochs to use (train using the entire train dataset epoch times.)\n",
    "                    callbacks=[checkpoint_callback]) # Set up the callback to save the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to practice the checkpoint API, lets load the trained weights into a new model and set its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_model()\n",
    "model2.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "The model.evaluate line below is the same as this handwritten loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_correct=0\n",
    "# for i in range(test_df.shape[0]):\n",
    "#     if round(model(test_df.iloc[i].to_numpy().reshape((1, 50))).numpy()[0][0]) == test_labels.iloc[i]:\n",
    "#         n_correct += 1\n",
    "# print(f'n_correct={n_correct}, n_correct/n_total={n_correct/test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_df.to_numpy(), test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But model.evaluate is optimized so it is MUCH quicker.\n",
    "\n",
    "## Plot the model accuracy and loss functions \n",
    "as a function of Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.set_ylim(0, 1) # set the vertical range to [0-1]"
   ]
  }
 ]
}